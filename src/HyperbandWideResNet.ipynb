{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperband training for our deep learning model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyperband algorithm randomly generates a set of configurations of hyperparameters and then tests them with some model on a given dataset. Initially, the tests are computed using very few iterations. The best performing 50% of the configurations are kept and the other 50% are resampled from the space of hyperparameters (successive halving).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "from PIL import Image, ImageOps\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.init as init\n",
    "from torchvision import transforms, utils\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from hyperopt import hp\n",
    "from hyperopt.pyll.stochastic import sample\n",
    "import pickle\n",
    "from collections import OrderedDict\n",
    "import math\n",
    "from IPython.display import Image\n",
    "\n",
    "# Custom generator for our dataset\n",
    "from modules.dataset import PileupDataset, TextColor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of Hyperband, modified for python3. Copied from [here](https://github.com/elPistolero/hyperband_pytorch_example/blob/master/Hyperband%20Tutorial%20for%20PyTorch.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from random import random\n",
    "from math import log, ceil\n",
    "from time import time, ctime\n",
    "\n",
    "\n",
    "class Hyperband:\n",
    "    \n",
    "    def __init__( self, get_params_function, try_params_function ):\n",
    "        self.get_params = get_params_function\n",
    "        self.try_params = try_params_function\n",
    "        \n",
    "        self.max_iter = 20      # maximum iterations per configuration\n",
    "        self.eta = 3            # defines configuration downsampling rate (default = 3)\n",
    "\n",
    "        self.logeta = lambda x: log( x ) / log( self.eta )\n",
    "        self.s_max = int( self.logeta( self.max_iter ))\n",
    "        self.B = ( self.s_max + 1 ) * self.max_iter\n",
    "\n",
    "        self.results = []    # list of dicts\n",
    "        self.counter = 0\n",
    "        self.best_loss = np.inf\n",
    "        self.best_counter = -1\n",
    "        \n",
    "\n",
    "    # can be called multiple times\n",
    "    def run( self, skip_last = 0, dry_run = False ):\n",
    "        \n",
    "        for s in reversed( range( self.s_max + 1 )):\n",
    "            \n",
    "            # initial number of configurations\n",
    "            n = int( ceil( self.B / self.max_iter / ( s + 1 ) * self.eta ** s ))    \n",
    "            \n",
    "            # initial number of iterations per config\n",
    "            r = self.max_iter * self.eta ** ( -s )        \n",
    "\n",
    "            # n random configurations\n",
    "            T = [ self.get_params() for i in range( n )] \n",
    "            \n",
    "            for i in range(( s + 1 ) - int( skip_last )):    # changed from s + 1\n",
    "                \n",
    "                # Run each of the n configs for <iterations> \n",
    "                # and keep best (n_configs / eta) configurations\n",
    "                \n",
    "                n_configs = n * self.eta ** ( -i )\n",
    "                n_iterations = r * self.eta ** ( i )\n",
    "                \n",
    "                print(\"\\n*** {} configurations x {:.1f} iterations each\".format( n_configs, n_iterations ))\n",
    "                \n",
    "                val_losses = []\n",
    "                early_stops = []\n",
    "                \n",
    "                for t in T:\n",
    "                    \n",
    "                    self.counter += 1\n",
    "                    print(\"\\n{} | {} | lowest loss so far: {:.4f} (run {})\\n\".format( self.counter, ctime(), self.best_loss, self.best_counter ))\n",
    "                    \n",
    "                    start_time = time()\n",
    "                    \n",
    "                    if dry_run:\n",
    "                        result = { 'loss': random(), 'log_loss': random(), 'auc': random()}\n",
    "                    else:\n",
    "                        result = self.try_params( n_iterations, t )        # <---\n",
    "                        \n",
    "                    assert( type( result ) == dict )\n",
    "                    assert( 'loss' in result )\n",
    "                    \n",
    "                    seconds = int( round( time() - start_time ))\n",
    "                    print(\"\\n{} seconds.\".format( seconds))\n",
    "                    \n",
    "                    loss = result['loss']    \n",
    "                    val_losses.append( loss )\n",
    "                    \n",
    "                    early_stop = result.get( 'early_stop', False )\n",
    "                    early_stops.append( early_stop )\n",
    "                    \n",
    "                    # keeping track of the best result so far (for display only)\n",
    "                    # could do it be checking results each time, but hey\n",
    "                    if loss < self.best_loss:\n",
    "                        self.best_loss = loss\n",
    "                        self.best_counter = self.counter\n",
    "                    \n",
    "                    result['counter'] = self.counter\n",
    "                    result['seconds'] = seconds\n",
    "                    result['params'] = t\n",
    "                    result['iterations'] = n_iterations\n",
    "                    \n",
    "                    self.results.append( result )\n",
    "                \n",
    "                # select a number of best configurations for the next loop\n",
    "                # filter out early stops, if any\n",
    "                indices = np.argsort( val_losses )\n",
    "                T = [ T[i] for i in indices if not early_stops[i]]\n",
    "                T = T[ 0:int( n_configs / self.eta )]\n",
    "        \n",
    "        return self.results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The network- Wide Residual Networks\n",
    "### Re-implemented to fit our problem space\n",
    "Deep residual networks were shown to be able to scale up to thousands of layers and still have improving performance. However, each fraction of a percent of improved accuracy costs nearly doubling the number of layers, and so training very deep residual networks has a problem of diminishing feature reuse, which makes these networks very slow to train.\n",
    "\n",
    "To tackle these problems, in this work we conduct a detailed experimental study on the architecture of ResNet blocks, based on which we propose a novel architecture where we decrease depth and increase width of residual networks. We call the resulting network structures wide residual networks (WRNs) and show that these are far superior over their commonly used thin and very deep counterparts."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a block of the neural network\n",
    "![Block.png](./img/Block.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Single block defining a single cell in the network\n",
    "class SingleBlock(nn.Module):\n",
    "    '''\n",
    "    Structure of a single block/cell in a layer. Consider this as a single neuron of the network.\n",
    "    '''\n",
    "    def __init__(self, in_channels, out_channels, stride, drop_rate=0.0):\n",
    "        '''\n",
    "        Initialize a single cell.\n",
    "        '''\n",
    "        super(SingleBlock, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_channels)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1,\n",
    "                               padding=1, bias=False)\n",
    "        self.drop_rate = drop_rate\n",
    "        self.equalInOut = (in_channels == out_channels)\n",
    "        self.convShortcut = (not self.equalInOut) and nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride,\n",
    "                               padding=0, bias=False) or None\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Perform all caluclations that happen in a single cell block.\n",
    "        '''\n",
    "        if not self.equalInOut:\n",
    "            x = self.relu1(self.bn1(x))\n",
    "        else:\n",
    "            out = self.relu1(self.bn1(x))\n",
    "        out = self.relu2(self.bn2(self.conv1(out if self.equalInOut else x)))\n",
    "        if self.drop_rate > 0:\n",
    "            out = F.dropout(out, p=self.drop_rate, training=self.training)\n",
    "        out = self.conv2(out)\n",
    "        return torch.add(x if self.equalInOut else self.convShortcut(x), out)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a layer of the network\n",
    "A layer consists of multiple blocks\n",
    "![Layer.png](./img/Layer.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LayerBlock(nn.Module):\n",
    "    '''\n",
    "    Structure of layer of the neural network. A layer would contain N number of blocks.\n",
    "    '''\n",
    "    def __init__(self, nb_layers, in_channels, out_channels, block, stride, drop_rate=0.0):\n",
    "        '''\n",
    "        Initialize a layer. Block is a cell represented as an object of SingleBlock class\n",
    "        '''\n",
    "        super(LayerBlock, self).__init__()\n",
    "        self.layer = self._make_layer(block, in_channels, out_channels, nb_layers, stride, drop_rate)\n",
    "\n",
    "    def _make_layer(self, block, in_channels, out_channels, nb_layers, stride, drop_rate):\n",
    "        '''\n",
    "        '''\n",
    "        layers = []\n",
    "        for i in range(nb_layers):\n",
    "            layers.append(block(i == 0 and in_channels or out_channels, out_channels, i == 0 and stride or 1, drop_rate))\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layer(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the network\n",
    "Define the whole network that we need to use\n",
    "![only_resnet.png](./img/only_resnet.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    '''\n",
    "    The model itself. It's a three block model with variable depth and widen factor.\n",
    "    '''\n",
    "    def __init__(self, input_channels, depth, num_classes, widen_factor, drop_rate):\n",
    "        super(Model, self).__init__()\n",
    "        nChannels = [16, 16*widen_factor, 32*widen_factor, 64*widen_factor]\n",
    "        assert((depth - 4) % 6 == 0)\n",
    "        n = int((depth - 4) / 6)\n",
    "        block = SingleBlock\n",
    "        # 1st conv before any network block\n",
    "        self.conv1 = nn.Conv2d(input_channels, nChannels[0], kernel_size=(1, 15), stride=1,\n",
    "                               padding=(0, 7), bias=False)\n",
    "        self.conv2 = nn.Conv2d(nChannels[0], nChannels[0], kernel_size=(3, 3), stride=1,\n",
    "                               padding=(1, 1), bias=False)\n",
    "        # 1st block\n",
    "        self.block1 = LayerBlock(n, nChannels[0], nChannels[1], block, 1, drop_rate)\n",
    "        # 2nd block\n",
    "        self.block2 = LayerBlock(n, nChannels[1], nChannels[2], block, (1, 2), drop_rate)\n",
    "        # 3rd block\n",
    "        self.block3 = LayerBlock(n, nChannels[2], nChannels[3], block, (1, 2), drop_rate)\n",
    "        # global average pooling and classifier\n",
    "        self.bn1 = nn.BatchNorm2d(nChannels[3])\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.fc = nn.Linear(nChannels[3] * 19, num_classes)\n",
    "        self.nChannels = nChannels[3]\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        seq_len = x.size(2)\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        out = self.block1(out)\n",
    "        out = self.block2(out)\n",
    "        out = self.block3(out)\n",
    "        out = self.relu(self.bn1(out))\n",
    "        out = F.avg_pool2d(out, (1, 2))\n",
    "        out = out.view(batch_size, seq_len, -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "Train the model with this method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(train_file, depth, widen_factor, drop_rate, batch_size, epoch_limit, learning_rate, l2, debug_mode, gpu_mode, seq_len, iteration_jump, num_classes):\n",
    "    \n",
    "    # Convert the image to tensor\n",
    "    transformations = transforms.Compose([transforms.ToTensor()])\n",
    "    \n",
    "    \n",
    "    if debug_mode:\n",
    "        sys.stderr.write(TextColor.PURPLE + 'Loading data\\n' + TextColor.END)\n",
    "    # Load training data\n",
    "    train_data_set = PileupDataset(train_file, transformations)\n",
    "    train_loader = DataLoader(train_data_set,\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=True,\n",
    "                              num_workers=16,\n",
    "                              pin_memory=gpu_mode\n",
    "                              )\n",
    "    if debug_mode:\n",
    "        sys.stderr.write(TextColor.PURPLE + 'Data loading finished\\n' + TextColor.END)\n",
    "    \n",
    "\n",
    "    # Create the model to train\n",
    "    input_channels = 3\n",
    "    model = Model(input_channels, depth, num_classes, widen_factor, drop_rate)\n",
    "    if gpu_mode:\n",
    "        model = torch.nn.DataParallel(model).cuda()\n",
    "\n",
    "    # Loss and Optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=l2)\n",
    "\n",
    "    # Train the Model\n",
    "    if debug_mode:\n",
    "        sys.stderr.write(TextColor.PURPLE + 'Training starting\\n' + TextColor.END)\n",
    "    \n",
    "    total_loss = 0\n",
    "    total_images = 0\n",
    "    \n",
    "    for epoch in range(epoch_limit):\n",
    "        \n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            # If batch size not distributable among all GPUs then skip\n",
    "            if gpu_mode is True and images.size(0) % 8 != 0:\n",
    "                continue\n",
    "                \n",
    "            # Take images and labels of batch_size to train\n",
    "            images = Variable(images, requires_grad=False)\n",
    "            labels = Variable(labels, requires_grad=False)\n",
    "            if gpu_mode:\n",
    "                images = images.cuda()\n",
    "                labels = labels.cuda()\n",
    "                \n",
    "            # Go through the window size of the image\n",
    "            for row in range(0, images.size(2), iteration_jump):\n",
    "                # Segmentation of image to window size. Currently using seq_len\n",
    "                if row+seq_len > images.size(2):\n",
    "                    continue\n",
    "\n",
    "                x = images[:, :, row:row+seq_len, :]\n",
    "                y = labels[:, row:row + seq_len]\n",
    "                \n",
    "                # Subsampling step, if there's less evidence of training on het, hom-alt then don't train\n",
    "                total_variation = torch.sum(y.eq(2)).data[0]\n",
    "                total_variation += torch.sum(y.eq(3)).data[0]\n",
    "\n",
    "                if total_variation == 0 and np.random.uniform(0, 1)*100 > 5:\n",
    "                    continue\n",
    "                elif np.random.uniform(0, 1) < total_variation/batch_size < 0.02:\n",
    "                    continue\n",
    "\n",
    "                # Forward + Backward + Optimize\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(x)\n",
    "\n",
    "                loss = criterion(outputs.contiguous().view(-1, 4), y.contiguous().view(-1))\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                # Add up loss and total trianing set\n",
    "                total_loss += loss.data[0]\n",
    "                total_images += (x.size(0)*seq_len)\n",
    "                \n",
    "            if debug_mode:\n",
    "                sys.stderr.write(TextColor.BLUE + \"EPOCH: \" + str(epoch) + \" Batches done: \" + str(i+1))\n",
    "                sys.stderr.write(\" Loss: \" + str(total_loss/total_images) + \"\\n\" + TextColor.END)\n",
    "                # print(str(epoch) + \"\\t\" + str(i + 1) + \"\\t\" + str(total_loss/total_images))\n",
    "\n",
    "        # After each epoch print loss and save model if debug mode is on\n",
    "        if debug_mode:\n",
    "            sys.stderr.write(TextColor.YELLOW + 'EPOCH: ' + str(epoch))\n",
    "            sys.stderr.write(' Loss: ' + str(total_loss/total_images) + \"\\n\" + TextColor.END)\n",
    "    \n",
    "    if debug_mode:\n",
    "        sys.stderr.write(TextColor.PURPLE + 'Finished training\\n' + TextColor.END)\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_model(model, file_name):\n",
    "    '''\n",
    "    Not used in hyperband, but this method saves trained models on disk.\n",
    "    '''\n",
    "    torch.save(model, file_name+'_final.pkl')\n",
    "\n",
    "    sys.stderr.write(TextColor.PURPLE + 'Model saved as:' + file_name + '.pkl\\n' + TextColor.END)\n",
    "    torch.save(model.state_dict(), file_name+'_final_params'+'.pkl')\n",
    "\n",
    "    sys.stderr.write(TextColor.PURPLE + 'Model parameters saved as:' + file_name + '-params.pkl\\n' + TextColor.END)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test\n",
    "Test a trained model. Return test loss and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def most_common(lst):\n",
    "    return max(set(lst), key=lst.count)\n",
    "\n",
    "def test(model, test_file, batch_size, num_classes, gpu_mode, seq_len, debug_mode):\n",
    "    # Convert the image to tensor\n",
    "    transformations = transforms.Compose([transforms.ToTensor()])\n",
    "    \n",
    "    if debug_mode:\n",
    "        sys.stderr.write(TextColor.PURPLE + 'Loading data\\n' + TextColor.END)\n",
    "        \n",
    "    #Load the dataset\n",
    "    test_dataset = PileupDataset(test_file, transformations)\n",
    "    testloader = DataLoader(test_dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=False,\n",
    "                            num_workers=16,\n",
    "                            pin_memory=gpu_mode # CUDA only\n",
    "                            )\n",
    "    \n",
    "    if debug_mode:\n",
    "        sys.stderr.write(TextColor.PURPLE + 'Data loading finished\\n' + TextColor.END)\n",
    "\n",
    "    if gpu_mode:\n",
    "        model = model.cuda()\n",
    "    \n",
    "    # Loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Change model to 'eval' mode (BN uses moving mean/var).\n",
    "    model.eval()  \n",
    "\n",
    "    # Create a confusion tensor for evaluation\n",
    "    confusion_tensor = torch.zeros(num_classes, num_classes)\n",
    "    test_loss = 0\n",
    "    total_datapoint = 0\n",
    "    # Start testing the model\n",
    "    for counter, (images, labels) in enumerate(testloader):\n",
    "        \n",
    "        # Load images and labels\n",
    "        images = Variable(images, volatile=True)\n",
    "        pl = labels\n",
    "        if gpu_mode:\n",
    "            images = images.cuda()\n",
    "        window = 1\n",
    "        \n",
    "        # We take vote between all predicted classes for a base, prediction_stack keeps track of the vote\n",
    "        prediction_stack = []\n",
    "        \n",
    "        # Per sequence window\n",
    "        for row in range(0, images.size(2), 1):\n",
    "            \n",
    "            # For now, don't thinking about trailing bases, they are always almost 0s\n",
    "            if row + seq_len > images.size(2):\n",
    "                continue\n",
    "                \n",
    "            \n",
    "            x = images[:, :, row:row + seq_len, :]\n",
    "            ypl = pl[:, row]\n",
    "            \n",
    "            # Get prediction in probability\n",
    "            preds = model(x)\n",
    "            labels = Variable(pl[:, row:row+seq_len], volatile=True)\n",
    "            \n",
    "            # Calculate the loss and keep track of it\n",
    "            loss = criterion(preds.contiguous().view(-1, num_classes), labels.contiguous().view(-1))\n",
    "            test_loss += loss.data[0]\n",
    "            total_datapoint += (seq_len * images.size(0))\n",
    "            \n",
    "            # Convert probability to class\n",
    "            preds = preds.data.topk(1)[1]\n",
    "            \n",
    "            # Append the generated prediction to stack so we can vote\n",
    "            prediction_stack.append(preds)\n",
    "            \n",
    "            # If we are at a position which is higher than window size then we can vote and predict\n",
    "            if row+1 >= seq_len:\n",
    "                # Go through each object in the stack\n",
    "                for i in range(images.size(0)):\n",
    "                    pr = []\n",
    "                    k = seq_len - 1\n",
    "                    # Collect all the votes\n",
    "                    for j in range(len(prediction_stack)):\n",
    "                        pr.append(prediction_stack[j][i][k][0])\n",
    "                        k-=1\n",
    "                    # The most frequent class wins and is the predicted class\n",
    "                    p = most_common(pr)\n",
    "                    # Get the target class\n",
    "                    t = ypl[i]\n",
    "                    # Update confusion tensor between target and predicted class\n",
    "                    confusion_tensor[t][p] += 1\n",
    "                    \n",
    "                # After we are done pop the top most vote cause we don't need that anymore\n",
    "                prediction_stack.pop(0)\n",
    "        #if debug_mode:\n",
    "            #print(confusion_tensor)\n",
    "    \n",
    "    correctly_predicted = torch.sum(confusion_tensor.diag())\n",
    "    total_datapoints = torch.sum(confusion_tensor)\n",
    "    accuracy = 100 * correctly_predicted / total_datapoints\n",
    "    total_test_loss = test_loss / total_datapoint\n",
    "    \n",
    "    if debug_mode:\n",
    "        print(confusion_tensor)\n",
    "    \n",
    "    return {'loss': total_test_loss, 'accuracy': accuracy}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The hyperparameter space\n",
    "\n",
    "First, we have to define the hyperparameter space, i.e. which hyperparameters are we interested in and how should they be sampled. We use hyperopt to define the search space.\n",
    "\n",
    "For our model, the parameters we are trying to learn: <br>\n",
    "1) Depth <br>\n",
    "2) Widen_factor <br>\n",
    "3) Batch_size <br>\n",
    "4) Drop_rate <br>\n",
    "5) Learning_rate <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Paramters of the model\n",
    "# depth=28 widen_factor=4 drop_rate=0.0\n",
    "space = {\n",
    "    'depth': hp.choice('depth', (10, 16)),\n",
    "    'widen_factor': hp.choice('widen_factor', (1, 2)),\n",
    "    'batch_size': hp.choice('batch_size', (5, 10)),\n",
    "    'dropout_rate': hp.quniform('dropout', 0, 0.5, 0.1),\n",
    "    'learning_rate': hp.loguniform('lr', -10, -2),\n",
    "    'l2': hp.loguniform('l2', -10, -2),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Parameter Method\n",
    "\n",
    "The first method we need to design for using hyperband is the get_parameter() method. This method returns parameters from sample space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_params():\n",
    "    return sample(space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Parameter Method\n",
    "\n",
    "The other method we need to design for hyperband is try_parameter(). In this method we try the parameter on the model we designed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Environment parameter, these parameters do not change learning or are fixed\n",
    "debug_mode = False\n",
    "gpu_mode = False\n",
    "num_classes=4\n",
    "train_file = \"~/Kishwar/pileup-output/toy_data/chr3/train.csv\"\n",
    "test_file = \"~/Kishwar/pileup-output/toy_data/chr3/test.csv\"\n",
    "\n",
    "# Few parameters are left out but can be tested\n",
    "seq_len = 3\n",
    "iteration_jump = 1\n",
    "\n",
    "def try_params(n_iterations, params):\n",
    "    # Number of iterations or epoch for the model to train on\n",
    "    n_iterations = int(round(n_iterations))\n",
    "    print(\"iterations: \", n_iterations)\n",
    "    print(\"params: \", params)\n",
    "    \n",
    "    \n",
    "    depth = params['depth']\n",
    "    widen_factor = params['widen_factor']\n",
    "    drop_rate = params['dropout_rate']\n",
    "    batch_size = params['batch_size']\n",
    "    epoch_limit = n_iterations\n",
    "    learning_rate = params['learning_rate']\n",
    "    l2 = params['l2']\n",
    "    \n",
    "    print('Training Model')\n",
    "    model = train(train_file, depth, widen_factor, drop_rate, batch_size, epoch_limit, learning_rate, l2, debug_mode, gpu_mode, seq_len, iteration_jump, num_classes)\n",
    "    print('Testing Model')\n",
    "    stats_dictionary = test(model, test_file, batch_size, num_classes, gpu_mode, seq_len, debug_mode)\n",
    "    print('Iteration Complete')\n",
    "    return stats_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** 9 configurations x 2.2 iterations each\n",
      "\n",
      "1 | Sat Nov 18 02:59:57 2017 | lowest loss so far: inf (run -1)\n",
      "\n",
      "iterations:  2\n",
      "params:  {'batch_size': 5, 'learning_rate': 0.06478659203641733, 'depth': 10, 'l2': 0.0006126154104723879, 'widen_factor': 1, 'dropout_rate': 0.4}\n",
      "Training Model\n",
      "Testing Model\n",
      "Iteration Complete\n",
      "\n",
      "12 seconds.\n",
      "\n",
      "2 | Sat Nov 18 03:00:09 2017 | lowest loss so far: 0.0156 (run 1)\n",
      "\n",
      "iterations:  2\n",
      "params:  {'batch_size': 10, 'learning_rate': 0.0011832055140218241, 'depth': 16, 'l2': 5.623791999752851e-05, 'widen_factor': 2, 'dropout_rate': 0.1}\n",
      "Training Model\n",
      "Testing Model\n",
      "Iteration Complete\n",
      "\n",
      "47 seconds.\n",
      "\n",
      "3 | Sat Nov 18 03:00:56 2017 | lowest loss so far: 0.0150 (run 2)\n",
      "\n",
      "iterations:  2\n",
      "params:  {'batch_size': 5, 'learning_rate': 0.014390163492011741, 'depth': 10, 'l2': 0.01553470439924714, 'widen_factor': 1, 'dropout_rate': 0.30000000000000004}\n",
      "Training Model\n",
      "Testing Model\n",
      "Iteration Complete\n",
      "\n",
      "13 seconds.\n",
      "\n",
      "4 | Sat Nov 18 03:01:10 2017 | lowest loss so far: 0.0150 (run 2)\n",
      "\n",
      "iterations:  2\n",
      "params:  {'batch_size': 10, 'learning_rate': 0.0004326796466266476, 'depth': 16, 'l2': 0.00936383360857324, 'widen_factor': 1, 'dropout_rate': 0.2}\n",
      "Training Model\n",
      "Testing Model\n",
      "Iteration Complete\n",
      "\n",
      "24 seconds.\n",
      "\n",
      "5 | Sat Nov 18 03:01:33 2017 | lowest loss so far: 0.0150 (run 2)\n",
      "\n",
      "iterations:  2\n",
      "params:  {'batch_size': 10, 'learning_rate': 0.023671114757606665, 'depth': 10, 'l2': 0.00011891216663045683, 'widen_factor': 1, 'dropout_rate': 0.2}\n",
      "Training Model\n",
      "Testing Model\n",
      "Iteration Complete\n",
      "\n",
      "14 seconds.\n",
      "\n",
      "6 | Sat Nov 18 03:01:47 2017 | lowest loss so far: 0.0095 (run 5)\n",
      "\n",
      "iterations:  2\n",
      "params:  {'batch_size': 10, 'learning_rate': 0.09733264067213353, 'depth': 10, 'l2': 0.0013677544465951921, 'widen_factor': 1, 'dropout_rate': 0.2}\n",
      "Training Model\n",
      "Testing Model\n",
      "Iteration Complete\n",
      "\n",
      "13 seconds.\n",
      "\n",
      "7 | Sat Nov 18 03:02:00 2017 | lowest loss so far: 0.0095 (run 5)\n",
      "\n",
      "iterations:  2\n",
      "params:  {'batch_size': 5, 'learning_rate': 7.856814449714523e-05, 'depth': 16, 'l2': 0.009216244907009626, 'widen_factor': 2, 'dropout_rate': 0.4}\n",
      "Training Model\n",
      "Testing Model\n",
      "Iteration Complete\n",
      "\n",
      "42 seconds.\n",
      "\n",
      "8 | Sat Nov 18 03:02:42 2017 | lowest loss so far: 0.0095 (run 5)\n",
      "\n",
      "iterations:  2\n",
      "params:  {'batch_size': 5, 'learning_rate': 0.031281959538054085, 'depth': 10, 'l2': 0.0578591881328504, 'widen_factor': 2, 'dropout_rate': 0.30000000000000004}\n",
      "Training Model\n",
      "Testing Model\n",
      "Iteration Complete\n",
      "\n",
      "24 seconds.\n",
      "\n",
      "9 | Sat Nov 18 03:03:06 2017 | lowest loss so far: 0.0095 (run 5)\n",
      "\n",
      "iterations:  2\n",
      "params:  {'batch_size': 5, 'learning_rate': 0.00012629089974537896, 'depth': 10, 'l2': 9.267846216685395e-05, 'widen_factor': 2, 'dropout_rate': 0.1}\n",
      "Training Model\n",
      "Testing Model\n",
      "Iteration Complete\n",
      "\n",
      "24 seconds.\n",
      "\n",
      "*** 3.0 configurations x 6.7 iterations each\n",
      "\n",
      "10 | Sat Nov 18 03:03:30 2017 | lowest loss so far: 0.0095 (run 5)\n",
      "\n",
      "iterations:  7\n",
      "params:  {'batch_size': 10, 'learning_rate': 0.023671114757606665, 'depth': 10, 'l2': 0.00011891216663045683, 'widen_factor': 1, 'dropout_rate': 0.2}\n",
      "Training Model\n",
      "Testing Model\n",
      "Iteration Complete\n",
      "\n",
      "33 seconds.\n",
      "\n",
      "11 | Sat Nov 18 03:04:04 2017 | lowest loss so far: 0.0062 (run 10)\n",
      "\n",
      "iterations:  7\n",
      "params:  {'batch_size': 10, 'learning_rate': 0.0011832055140218241, 'depth': 16, 'l2': 5.623791999752851e-05, 'widen_factor': 2, 'dropout_rate': 0.1}\n",
      "Training Model\n",
      "Testing Model\n",
      "Iteration Complete\n",
      "\n",
      "123 seconds.\n",
      "\n",
      "12 | Sat Nov 18 03:06:06 2017 | lowest loss so far: 0.0062 (run 10)\n",
      "\n",
      "iterations:  7\n",
      "params:  {'batch_size': 5, 'learning_rate': 0.06478659203641733, 'depth': 10, 'l2': 0.0006126154104723879, 'widen_factor': 1, 'dropout_rate': 0.4}\n",
      "Training Model\n",
      "Testing Model\n",
      "Iteration Complete\n",
      "\n",
      "29 seconds.\n",
      "\n",
      "*** 1.0 configurations x 20.0 iterations each\n",
      "\n",
      "13 | Sat Nov 18 03:06:35 2017 | lowest loss so far: 0.0062 (run 10)\n",
      "\n",
      "iterations:  20\n",
      "params:  {'batch_size': 10, 'learning_rate': 0.023671114757606665, 'depth': 10, 'l2': 0.00011891216663045683, 'widen_factor': 1, 'dropout_rate': 0.2}\n",
      "Training Model\n",
      "Testing Model\n",
      "Iteration Complete\n",
      "\n",
      "85 seconds.\n",
      "\n",
      "*** 5 configurations x 6.7 iterations each\n",
      "\n",
      "14 | Sat Nov 18 03:08:00 2017 | lowest loss so far: 0.0026 (run 13)\n",
      "\n",
      "iterations:  7\n",
      "params:  {'batch_size': 5, 'learning_rate': 0.0041705877514991285, 'depth': 10, 'l2': 0.0015791880946846271, 'widen_factor': 2, 'dropout_rate': 0.4}\n",
      "Training Model\n",
      "Testing Model\n",
      "Iteration Complete\n",
      "\n",
      "54 seconds.\n",
      "\n",
      "15 | Sat Nov 18 03:08:55 2017 | lowest loss so far: 0.0026 (run 13)\n",
      "\n",
      "iterations:  7\n",
      "params:  {'batch_size': 10, 'learning_rate': 0.0001997996933205312, 'depth': 10, 'l2': 0.00025974601156440724, 'widen_factor': 2, 'dropout_rate': 0.2}\n",
      "Training Model\n",
      "Testing Model\n",
      "Iteration Complete\n",
      "\n",
      "63 seconds.\n",
      "\n",
      "16 | Sat Nov 18 03:09:57 2017 | lowest loss so far: 0.0026 (run 13)\n",
      "\n",
      "iterations:  7\n",
      "params:  {'batch_size': 10, 'learning_rate': 0.016771915405873652, 'depth': 16, 'l2': 0.05205321336569357, 'widen_factor': 1, 'dropout_rate': 0.2}\n",
      "Training Model\n",
      "Testing Model\n",
      "Iteration Complete\n",
      "\n",
      "61 seconds.\n",
      "\n",
      "17 | Sat Nov 18 03:10:58 2017 | lowest loss so far: 0.0026 (run 13)\n",
      "\n",
      "iterations:  7\n",
      "params:  {'batch_size': 10, 'learning_rate': 0.016691568426023585, 'depth': 10, 'l2': 0.07868573460082534, 'widen_factor': 2, 'dropout_rate': 0.30000000000000004}\n",
      "Training Model\n",
      "Testing Model\n",
      "Iteration Complete\n",
      "\n",
      "62 seconds.\n",
      "\n",
      "18 | Sat Nov 18 03:12:00 2017 | lowest loss so far: 0.0026 (run 13)\n",
      "\n",
      "iterations:  7\n",
      "params:  {'batch_size': 10, 'learning_rate': 0.0004056679296886261, 'depth': 10, 'l2': 0.008782690695455423, 'widen_factor': 2, 'dropout_rate': 0.30000000000000004}\n",
      "Training Model\n",
      "Testing Model\n",
      "Iteration Complete\n",
      "\n",
      "66 seconds.\n",
      "\n",
      "*** 1.6666666666666665 configurations x 20.0 iterations each\n",
      "\n",
      "19 | Sat Nov 18 03:13:07 2017 | lowest loss so far: 0.0026 (run 13)\n",
      "\n",
      "iterations:  20\n",
      "params:  {'batch_size': 10, 'learning_rate': 0.0004056679296886261, 'depth': 10, 'l2': 0.008782690695455423, 'widen_factor': 2, 'dropout_rate': 0.30000000000000004}\n",
      "Training Model\n",
      "Testing Model\n",
      "Iteration Complete\n",
      "\n",
      "164 seconds.\n",
      "\n",
      "*** 3 configurations x 20.0 iterations each\n",
      "\n",
      "20 | Sat Nov 18 03:15:51 2017 | lowest loss so far: 0.0026 (run 13)\n",
      "\n",
      "iterations:  20\n",
      "params:  {'batch_size': 5, 'learning_rate': 0.0867456304616217, 'depth': 16, 'l2': 0.07600315758329258, 'widen_factor': 2, 'dropout_rate': 0.5}\n",
      "Training Model\n",
      "Testing Model\n",
      "Iteration Complete\n",
      "\n",
      "252 seconds.\n",
      "\n",
      "21 | Sat Nov 18 03:20:03 2017 | lowest loss so far: 0.0026 (run 13)\n",
      "\n",
      "iterations:  20\n",
      "params:  {'batch_size': 10, 'learning_rate': 7.74874240148753e-05, 'depth': 10, 'l2': 0.00024551655906534104, 'widen_factor': 2, 'dropout_rate': 0.2}\n",
      "Training Model\n",
      "Testing Model\n",
      "Iteration Complete\n",
      "\n",
      "155 seconds.\n",
      "\n",
      "22 | Sat Nov 18 03:22:38 2017 | lowest loss so far: 0.0026 (run 13)\n",
      "\n",
      "iterations:  20\n",
      "params:  {'batch_size': 5, 'learning_rate': 0.014411912868882154, 'depth': 10, 'l2': 0.0005383163524682256, 'widen_factor': 1, 'dropout_rate': 0.4}\n",
      "Training Model\n",
      "Testing Model\n",
      "Iteration Complete\n",
      "\n",
      "74 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'accuracy': 99.66216216216216,\n",
       "  'counter': 13,\n",
       "  'iterations': 20.0,\n",
       "  'loss': 0.0025684215525897417,\n",
       "  'params': {'batch_size': 10,\n",
       "   'depth': 10,\n",
       "   'dropout_rate': 0.2,\n",
       "   'l2': 0.00011891216663045683,\n",
       "   'learning_rate': 0.023671114757606665,\n",
       "   'widen_factor': 1},\n",
       "  'seconds': 85},\n",
       " {'accuracy': 99.51013513513513,\n",
       "  'counter': 10,\n",
       "  'iterations': 6.666666666666667,\n",
       "  'loss': 0.006199375358119498,\n",
       "  'params': {'batch_size': 10,\n",
       "   'depth': 10,\n",
       "   'dropout_rate': 0.2,\n",
       "   'l2': 0.00011891216663045683,\n",
       "   'learning_rate': 0.023671114757606665,\n",
       "   'widen_factor': 1},\n",
       "  'seconds': 33},\n",
       " {'accuracy': 98.83445945945945,\n",
       "  'counter': 5,\n",
       "  'iterations': 2.2222222222222223,\n",
       "  'loss': 0.009478952007032769,\n",
       "  'params': {'batch_size': 10,\n",
       "   'depth': 10,\n",
       "   'dropout_rate': 0.2,\n",
       "   'l2': 0.00011891216663045683,\n",
       "   'learning_rate': 0.023671114757606665,\n",
       "   'widen_factor': 1},\n",
       "  'seconds': 14},\n",
       " {'accuracy': 98.69932432432432,\n",
       "  'counter': 11,\n",
       "  'iterations': 6.666666666666667,\n",
       "  'loss': 0.010906192997206935,\n",
       "  'params': {'batch_size': 10,\n",
       "   'depth': 16,\n",
       "   'dropout_rate': 0.1,\n",
       "   'l2': 5.623791999752851e-05,\n",
       "   'learning_rate': 0.0011832055140218241,\n",
       "   'widen_factor': 2},\n",
       "  'seconds': 123},\n",
       " {'accuracy': 98.64864864864865,\n",
       "  'counter': 18,\n",
       "  'iterations': 6.666666666666666,\n",
       "  'loss': 0.011395422902330131,\n",
       "  'params': {'batch_size': 10,\n",
       "   'depth': 10,\n",
       "   'dropout_rate': 0.30000000000000004,\n",
       "   'l2': 0.008782690695455423,\n",
       "   'learning_rate': 0.0004056679296886261,\n",
       "   'widen_factor': 2},\n",
       "  'seconds': 66}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperband = Hyperband(get_params, try_params)\n",
    "results = hyperband.run()\n",
    "\n",
    "# Save results\n",
    "save_output = False\n",
    "\n",
    "if save_output:\n",
    "    with open('results.pkl', 'wb') as f:\n",
    "        pickle.dump(results, f)\n",
    "\n",
    "# Print top 5 configs based on loss\n",
    "sorted(results, key=lambda r: r['loss'])[:5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
